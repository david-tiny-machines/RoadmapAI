# v0.0.5g Implementation Plan: Retrieval-Augmented Generation (RAG) Integration

## Overview

This version builds upon `v0.0.5f` by integrating Retrieval-Augmented Generation (RAG) capabilities. When a user selects an *existing* initiative, the agent will now dynamically fetch specific details about that initiative from the database at relevant points during the conversation and use that information to enrich its prompts and generate more context-aware responses or questions.

1.  **Identify Retrieval Points:** Determine key conversational turns where initiative data (e.g., name, value lever, uplift, effort) is most relevant (e.g., asking about the problem, solution, metrics).
2.  **Conditional Data Retrieval:** Modify the backend API handler to fetch specific fields for the `selectedInitiativeId` (stored in session) before making certain OpenAI calls.
3.  **Prompt Augmentation:** Inject the retrieved initiative data into the OpenAI prompt (likely the system prompt or context for that specific turn).
4.  **Refined Prompt Engineering:** Update system prompts and potentially per-turn instructions to guide the LLM on how to effectively *use* the provided initiative data to tailor its questions or statements.

## Prerequisites

-   Completion and successful testing of `v0.0.5f` (Initiative Selection on Start).
-   Supabase database schema (`initiatives` table) includes fields relevant for retrieval (e.g., `name`, `value_lever`, `uplift`, `confidence`, `effort_estimate`).
-   Access to the application development environment (Replit).
-   Access to the codebase.
-   OpenAI API Key configured.
-   Supabase URL and Anon Key configured.

## Test Environment

-   Browser: Chrome latest version (or any modern browser).
-   User Accounts: Standard authenticated user access.
-   Supabase instance with test initiatives containing populated relevant fields.
-   Browser Developer Tools (Console tab).
-   Replit server console/logs.

## Database Updates

-   None required for schema. Assumes `initiatives` table has necessary fields.
-   **Data Requirement:** Test initiatives in the database should have values populated for fields like `value_lever`, `uplift`, `effort_estimate` for RAG to be meaningful.

## Component & Configuration Updates

1.  **API Route (`pages/api/agents/prd-generator.ts`):**
    *   **Action:** Modify the main handler logic, particularly within the "Normal Conversation Turn" section. Also update OpenAI model references.
    *   **Details:**
        *   **Update Model:** Change the `model` parameter in both `openai.chat.completions.create` calls (conversation and extraction) from `'gpt-4'` to `'gpt-4o'`.
        *   **Identify Trigger Points:** Determine which user inputs or which stage of the PRD flow should trigger data retrieval. For MVP RAG, this might be limited to specific transitions, e.g., *before* asking about the primary problem or *before* asking about key metrics.
        *   **Session Context Check:** Before making an OpenAI call at a trigger point, check `sessionData.selectedInitiativeId`. If it's set (i.e., not a "New" initiative), proceed with retrieval.
        *   **Targeted Supabase Query:** Perform an async Supabase query to fetch specific, relevant fields for the `sessionData.selectedInitiativeId`.
            ```typescript
            // Example: Fetching data before asking about metrics
            if (/* condition to fetch data */ && sessionData.selectedInitiativeId) {
                const { data: initiativeDetails, error: dbError } = await supabase
                    .from('initiatives')
                    .select('name, value_lever, uplift, confidence') // Fetch only relevant fields
                    .eq('id', sessionData.selectedInitiativeId)
                    .single(); // Expect only one result

                if (dbError) {
                    console.error(`[${sessionId}] Error fetching initiative details for RAG:`, dbError);
                    // Proceed without augmentation? Or return an error?
                } else if (initiativeDetails) {
                    console.log(`[${sessionId}] Fetched initiative details for RAG.`);
                    // Store or format details for prompt augmentation
                    retrievedRawContext = initiativeDetails; // Store for prompt injection
                }
            }
            ```
        *   **Prompt Construction:** Modify how the `messages` array is constructed for the OpenAI call. Inject the `retrievedRawContext` into the system prompt or as context.
            *   **Option A (Modify System Prompt):** Add a placeholder in the main `systemPrompt` content like `[Initiative Context Placeholder]` and replace it dynamically if data is retrieved.
            *   **Option B (Add Context Message):** Add a new `system` or `user` message just before the main user message, containing the retrieved data. E.g., `{ role: 'system', content: 'Context for current initiative: Name: ..., Value Lever: ...' }`.
            *   **Update Instructions:** Ensure the prompt clearly instructs the LLM on how to use this context (e.g., "Use the provided initiative context to ask the user about key success metrics related to the value lever.").
        *   **Handle Cases without Retrieval:** Ensure the flow works correctly if it's a "New" initiative or if the database query fails (the LLM call should proceed without the augmented context).

2.  **System Prompt (`systemPrompt` variable in `pages/api/agents/prd-generator.ts`):**
    *   **Action:** Potentially update the base system prompt.
    *   **Details:** Refine the instructions to mention that context about an existing initiative *might* be provided and how the assistant should use it when available.

## Success Criteria

1.  The OpenAI model used for conversation and extraction is explicitly set to `gpt-4o`.
2.  When a user selects an *existing* initiative, the agent's subsequent questions/responses at specific points (e.g., regarding metrics) incorporate details fetched from that initiative's database record (e.g., mentioning the `value_lever`).
3.  When a user selects "New", the conversation proceeds without attempting to fetch or inject initiative-specific data.
4.  If fetching initiative details fails (e.g., database error), the conversation gracefully continues without the augmented context.
5.  The core PRD generation flow and `/done` functionality remain intact for both existing and new initiatives.
6.  Logging clearly indicates when initiative data is being fetched and potentially injected for RAG.

## Manual Testing Steps

### 1. Setup
-   Ensure at least 2-3 initiatives exist in the `initiatives` table.
-   **Crucially:** Populate fields like `value_lever`, `uplift`, `confidence`, `effort_estimate` for at least one of these test initiatives with distinct, recognizable values.

### 2. RAG Flow (Existing Initiative)
-   **Test 2.1:** Select Initiative with Data
    -   **Action:** Start a new conversation. Select the initiative that has populated data fields.
    -   **Expected Result:** Initial selection works as per v0.0.5f.
    -   **Status:** TBD
-   **Test 2.2:** Contextual Question (e.g., Metrics)
    -   **Action:** Proceed through the conversation until the point where RAG is expected (e.g., when the agent asks about Key Success Metrics).
    -   **Expected Result:** The agent's question should be tailored using data from the selected initiative. For example, instead of just "What are the Key Success Metrics?", it might say something like, "Okay, for 'Initiative Name X', which aims to impact '[Value Lever]' with an estimated uplift of [Uplift]%, what Key Success Metrics will you track?"
    -   **Status:** TBD
-   **Test 2.3:** Verify Server Logs
    -   **Action:** Check the Replit server logs during Test 2.2.
    -   **Expected Result:** Logs should show the attempt to fetch initiative details for RAG and potentially the data being injected (or a snippet of the augmented prompt).
    -   **Status:** TBD
-   **Test 2.4:** Complete Flow
    -   **Action:** Continue the conversation and use `/done`.
    -   **Expected Result:** PRD generation completes successfully.
    -   **Status:** TBD

### 3. Non-RAG Flow (New Initiative)
-   **Test 3.1:** Select "New" Initiative
    -   **Action:** Start a new conversation. Select "New".
    -   **Expected Result:** Initial selection works.
    -   **Status:** TBD
-   **Test 3.2:** Non-Contextual Question
    -   **Action:** Proceed through the conversation to the same point as Test 2.2 (e.g., asking about metrics).
    -   **Expected Result:** The agent should ask the generic question (e.g., "What are the Key Success Metrics?") without referencing specific initiative data.
    -   **Status:** TBD
-   **Test 3.3:** Verify Server Logs
    -   **Action:** Check the Replit server logs during Test 3.2.
    -   **Expected Result:** Logs should *not* show attempts to fetch initiative details for RAG.
    -   **Status:** TBD
-   **Test 3.4:** Complete Flow
    -   **Action:** Continue the conversation and use `/done`.
    -   **Expected Result:** PRD generation completes successfully.
    -   **Status:** TBD

### 4. Edge Case: RAG Data Fetch Fails
-   **Test 4.1:** Simulate DB Error (if possible/practical)
    -   **Action:** Start conversation, select an existing initiative. Find a way to temporarily cause the Supabase query within the RAG logic to fail (e.g., temporarily revoke permissions if easy, or modify code to force an error condition).
    -   **Expected Result:** The agent should gracefully handle the error (log it server-side) and proceed with the conversation using the non-augmented prompt/question, similar to the "New" initiative flow.
    -   **Status:** TBD 